{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include DomainML src in module path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass issues with invoking notebook with server arguments\n",
    "sys.argv.clear()\n",
    "sys.argv.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "tensorflow_seed = 7796\n",
    "random_seed = 82379498237\n",
    "\n",
    "tf.random.set_seed(tensorflow_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import preprocessing\n",
    "from src.features.preprocessing import mimic\n",
    "\n",
    "preprocessor_config = mimic.MimicPreprocessorConfig()\n",
    "preprocessor_config.prediction_column = \"level_0\"\n",
    "preprocessor_config.sequence_column_name = \"level_all\"\n",
    "\n",
    "sequences_df = preprocessing.MimicPreprocessor(preprocessor_config).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect sequence metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import sequences\n",
    "from src.features.sequences import transformer\n",
    "\n",
    "sequence_column_name = preprocessor_config.sequence_column_name\n",
    "\n",
    "transformer_config = sequences.SequenceConfig()\n",
    "transformer_config.x_sequence_column_name = \"level_0\"\n",
    "transformer_config.y_sequence_column_name = \"level_3\"\n",
    "transformer_config.predict_full_y_sequence_wide = True\n",
    "\n",
    "transformer = transformer.NextPartialSequenceTransformerFromDataframe(transformer_config)\n",
    "\n",
    "metadata = transformer.collect_metadata(sequences_df, sequence_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot use sequences.generate_train or sequences.generate_test because they internally use sequences.load_sequence_transformer, which will load the incorrect transformer in our case\n",
    "def generate(for_train):\n",
    "    train_sequences, test_sequences = transformer._split_train_test(sequences_df, sequence_column_name)\n",
    "    relevant_sequences = train_sequences if for_train else test_sequences\n",
    "\n",
    "    for sequence in relevant_sequences:\n",
    "        split_sequences = transformer._split_sequence(sequence)\n",
    "        for split_sequence in split_sequences:\n",
    "            transformer._translate_and_pad(split_sequence, metadata)\n",
    "            yield split_sequence.x_vecs_stacked, split_sequence.y_vec\n",
    "\n",
    "def generate_train():\n",
    "    return generate(for_train=True)\n",
    "\n",
    "def generate_test():\n",
    "    return generate(for_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shuffle_buffer = 1000\n",
    "dataset_shuffle_seed = 12345\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_generator(generate_train, output_types=(tf.float32, tf.float32))\n",
    ".shuffle(dataset_shuffle_buffer, dataset_shuffle_seed, reshuffle_each_iteration=True)\n",
    ".batch(batch_size)\n",
    ".prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_generator(generate_test,\n",
    "output_types=(tf.float32, tf.float32))\n",
    ".batch(batch_size)\n",
    ".prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import knowledge\n",
    "\n",
    "hierarchy_df = preprocessing.ICD9HierarchyPreprocessor(preprocessor_config).load_data()\n",
    "hierarchy = knowledge.HierarchyKnowledge(knowledge.KnowledgeConfig())\n",
    "hierarchy.build_hierarchy_from_df(hierarchy_df, metadata.x_vocab)\n",
    "\n",
    "from src.training import models\n",
    "\n",
    "model = models.GramModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(metadata, hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_classification = False\n",
    "n_epochs = 1\n",
    "\n",
    "model.train_dataset(train_dataset, test_dataset, multilabel_classification, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36913b3a8fdee247ab31c445bb99cd1b610e5fce5968ec944dddf0bd9ac2faba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('healthcare-aiops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
